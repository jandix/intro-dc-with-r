---
title: "Session 6 - APIs"
subtitle: "An Introduction to Data Collection with R"
author: "Philipp Behrendt & Jan Dix"
date: "05. Juni 2018"
output: 
  ioslides_presentation:
    slide_level: 2
    logo: ../Assets/img/logo-uni.png
    widescreen: true
    css: ../Assets/css/style.css
---

```{r setup, include=FALSE, echo=F, error=F, warning=F, message=F}
knitr::opts_chunk$set(echo = TRUE)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())

# load required packages
library(tidyverse)
library(jsonlite)
library(xml2)
library(httr)
library(ggmap)
library(rtimes)
library(rtweet)
```

## Syllabus

Session 1 - R and RStudio

Session 2 - Data Typen und erste Befehle

Session 3 - Daten einlesen und speichern

Session 4 - Einfache Analyse (numerisch)

Session 5 - Visualisierung

`Session 6 & 7 - Application Programming Interfaces (APIs)`

Session 8 - Introduction to Web Scraping

Session 9 - Intorduction to Text Mining


## Agenda

1. Missing Values
1. Datenformate
2. Aufgaben
3. Was sind in APIs?
4. Aufgaben
5. OpenStreetMap API
6. Aufgaben
7. New York Times API
8. Twitter API
9. Hausaufgaben

# Missing values

## Missing values (1)

```{r, eval=FALSE}
# löschen aller Zeilen, die mindestens ein NA haben
gtdb_complete <- na.omit(gtdb)
```

**<i class="fa fa-exclamation-triangle "></i> Obacht:**

Bias Gefahr! Es ist möglich, dass Werte auf Grund einer Drittvariablen nicht beobachtet werden können. Normalerweise sollten fehlende Observationen genau untersucht und begründet werden.

# Datenformate

## JSON (1)

```{json}
[
  {
    "name": "Batman",
    "age": 99
  },
  {
    "name": "Robin",
    "age": 101
  }
]
```

## JSON (2)

```{r, echo=FALSE}
json <- '[
  {
    "name": "Batman",
    "age": 99
  },
  {
    "name": "Robin",
    "age": 101
  }
]'
parsed_json <- fromJSON(json)
xml <- '<persons>
  <person name="Batman" age="99"></person>
  <person name="Robin" age="102"></person>
</persons>'
parsed_xml <- read_xml(xml)
parsed_persons <- xml_find_all(parsed_xml, "//person")
parsed_text <- xml_attr(parsed_persons, "name")
```

```{r, eval=FALSE}
# parsing (lesen) json
library(jsonlite)
json <- readLines("batman.json")
parsed_json <- fromJSON(json)
```
```{r, echo=FALSE}
parsed_json
```

## XML (1)

```{xml}
<persons>
  <person name="Batman" age="99"></person>
  <person name="Robin" age="102"></person>
</persons>
```

## XML (2)

```{r, eval=FALSE}
# parsing (lesen) xml
library(xml2)
xml <- readLines("batman.xml")
parsed_xml <- read_xml(xml)
parsed_persons <- xml_find_all(parsed_xml, "//person")
parsed_text <- xml_attr(parsed_persons, "name")
```
```{r, echo=FALSE}
parsed_text
```

## Aufgaben

Textdateien können mit Hilfe des Befehls `readLines()` eingelesen werden. Die Funktion nimmt als Argument einen Dateipfad oder eine URL.

1. Lese und parse die Datei [https://raw.githubusercontent.com/jandix/intro-dc-with-r/master/Data/Session_6/batman.xml](https://raw.githubusercontent.com/jandix/intro-dc-with-r/master/Data/Session_6/batman.xml).
2. Lese und parse die Datei [https://raw.githubusercontent.com/jandix/intro-dc-with-r/master/Data/Session_6/countries.json](https://raw.githubusercontent.com/jandix/intro-dc-with-r/master/Data/Session_6/countries.json) und gebe das Alter aller _Heroes_ aus. Errechne anschließend das Durchschnittsalter.

# Was sind APIs?

## Was sind APIs? (1)

```{r, out.width = "65%",echo=FALSE, fig.align="center"}
knitr::include_graphics("../Assets/img/session6/api.png", dpi = 300)
```

## Was sind APIs? (2)

```{r, out.width = "100%",echo=FALSE}
knitr::include_graphics("../Assets/img/session6/url.png")
```

**BASE URL** (Stammurl): Wer bietet den Service an?

**ENDPOINT** (Endpunkt): Welchen Service möchtest du nutzten?

**PARAMETER** (Query): Auf welche Daten möchtest du zugreifen? Wie sollen die Daten gefiltert werden? 

## Aufgaben

Unter [https://wiki.openstreetmap.org/wiki/Nominatim](https://wiki.openstreetmap.org/wiki/Nominatim) findest du die Dokumentation der OpenStreetMap Nominatim API. Du kannst die Aufgaben mit Hilfe des Browsers lösen.

1. Welche Koordinaten hat die Universität Konstanz?
2. Zu welchem `County` gehört die Universität Harvard? 
3. Die API kann sowohl die Daten im `JSON` und `XML` Format ausgeben. Wie kann man das Format ändern? Welches Format ist besser?

# APIs in R

## OSM Nominatim (1)

**<i class="fa fa-exclamation-triangle "></i> Wir benutzen OpenStreetMap, da der Google Map Service einen Key benötigt.**


```{r, eval=FALSE}
# Addresse zu Koordinaten
osm_geocoding()
# Koordinaten zu Addresse
osm_reverse_geocoding()
```

Die Funktionen sind nicht in Base R verfügbar, sondern müssen aus dem Script `openstreetmap.r` geladen werden.

```{r}
source("openstreetmap.r")
```

## OSM Nominatim (2)

```{r}
# Koordinaten von Konstanz
coordinates_kn <- osm_geocoding("Konstanz")
coordinates_kn[ , 6:7]
```
```{r}
# Ort an den Koordinaten Lat, Lon
lat <- 38.897096
lon <- -77.036545
secret_place <- osm_reverse_geocoding(lat, lon)
secret_place[ , 1:3]
```

## Aufgaben

Erstelle ein Sample mit allen vollständigen Observationen aus dem Jahr **2012**. Speicher diesen Datensatz in `gtdb_2012_complete`. 

1. Erstelle eine leere Spalte `country` in dem neuen Datensatz.
2. Ermittle den **country_code** für jede Observation mit Hilfe von `osm_reverse_geocoding()`. **Obacht:** Diese Funktion ist nicht vektorisiert. Die Ausführung kann sehr, sehr lange dauern.


## rtimes (1)

- Key beantragen: [https://developer.nytimes.com/signup](https://developer.nytimes.com/signup)

```{r, message=F, eval=F}
# Paket laden
library(rtimes)
# Artikel suchen mit dem Stichwort Merkel
article <- as_search("Merkel", 
                     begin_date = "20150101",
                     end_date = "20151231",
                     key = "***")
article$data
```


```{r, echo=FALSE, cache=TRUE}
articles <- as_search("Merkel", 
                      begin_date = "20150101",
                      end_date = "20151231")
articles$data[1:3, c("headline.main", "new_desk", "word_count")]
```

## rtweet (1)

- App erstellen und Key erhalten: [https://apps.twitter.com/](https://apps.twitter.com/)
- Dokumentation und Beispiele: [http://rtweet.info/](http://rtweet.info/)

```{r, message=F, eval=F}
# Paket laden
library(rtweet)
# namen deiner App 
app_name <- "app_name"
# api key 
key <- "***"
# api secret
secret <- "***"
# erstelle den token
twitter_token <- create_token(app = appname,
                              consumer_key = key,
                              consumer_secret = secret)

```

## rtweet (2)

```{r, eval=FALSE}
tweets <- search_tweets("@realdonaldtrump", 
                        n = 2000,
                        token = twitter_token)
tweets
```

```{r, echo=FALSE, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE}
tweets <- search_tweets("Trump", 
                        n = 2000)
tweets[1:3, c("screen_name", "text")]
```



## Aufgaben

1. Erstelle ein Sample basierend auf einem Land (z.B.: Kolumbien).
2. Überlege dir eine Strategie, um für jedes Ereignis in deinem Sample, Zeitungsartikel in der New York Times API zu finden. Kombiniere dazu geschickt **Zeit**, **attack_type** und Land.
3. Speicher alle Metadaten zu den Zeitungsartikeln in einem neuen `data.frame` mit dem Namen `nyt_articles`. **Obacht:** Da du mehere Abfragen startest, solltest du die EventID in einer zusätzlichen Spalte speichern, um die Artikel dem Event zuordnen zu können.  




